h1. Swineherd

h3. Script

Given the Pig script @foo.pig.erb@:
<pre><code>fips = LOAD '<%= in_path %>' AS (fips_id:int,state_name:chararray);
DUMP fips;
</code></pre>

In the Ruby interpreter (irb):
<pre><code>require 'swineherd'
script = Swineherd::Script.new('foo.pig.erb',:in_path => 'fips_to_state.tsv')
script.run(:map_tasks => 20,:run_mode => 'local') #uses the PigRunner based upon the filename extension
</code></pre>

Hadoop and Pig Jobconf settings can be specified at runtime or at a system/user wide level:

@/etc/swineherd.yaml@
<pre><code>:map_tasks: 10</code></pre>
@~/swineherd.yaml@
<pre><code>:map_tasks:15</code></pre>

The above @script.run@ will interpolate the script variables,pass Hadoop and Pig settings through @PIG_OPTS@ and run the following script:

@/tmp/1325544799-4248-foo.pig@
<pre><code>fips = LOAD 'fips_to_state.tsv' AS (fips_id:int,state_name:chararray);
DUMP fips;
</code></pre>

With this command line:

<pre><code>ENV['PIG_OPTS'] = '-Dmapred.map.tasks=20'
/usr/local/share/pig/bin/pig -x local /tmp/1325544799-4248-foo.pig
</code></pre>

The same script can be run using the swineherd executable:

<pre><code>./swineherd --map_tasks=20 --run_mode=local --binding.in_path=fips_to_state.tsv foo.pig.erb</code></pre>

Script variables are specified using @--binding@

h3. FileSystem

    * @file@ - Local file system. Only thoroughly tested on Ubuntu Linux.
    * @hdfs@ - Hadoop distributed file system. Uses the Apache Hadoop 0.20 API. Requires Jruby.
    * @s3@   - Amazon Simple Storage System (s3).
    * @ftp@  - FTP (Not yet implemented)

    All filesystem abstractions implement the following core functions, many taken from the UNIX filesystem:

    * @mv@
    * @cp@
    * @cp_r@
    * @rm@
    * @rm_r@
    * @open@
    * @exists?@
    * @directory?@
    * @ls@
    * @ls_r@
    * @mkdir_p@

    Note: Since S3 is just a key-value store, it is difficult to preserve the notion of a directory.  Therefore the @mkdir_p@ function has no purpose, as there cannot be empty directories. @mkdir_p@ currently only ensures that the bucket exists.  This implies that the @directory?@ test only succeeds if the directory is non-empty, which clashes with the notion on the UNIX filesystem.

    Additionally, the S3 and HDFS abstractions implement functions for moving files to and from the local filesystem:

    * @copy_to_local@  
    * @copy_from_local@

    Note: For these methods the destination and source path respectively are assumed to be local, so they do not have to be prefaced by a filescheme.
    
    The @Swineherd::Filesystem@ module implements a generic filesystem abstraction using schemed filepaths (hdfs://,s3://,file://).

    Currently only the following methods are supported for @Swineherd::Filesystem@:

    * @cp@
    * @exists?@
    
    For example, instead of doing the following:<pre><code>hdfs = Swineherd::HadoopFilesystem.new
    localfs = Swineherd::LocalFileSystem.new
    hdfs.copy_to_local('foo/bar/baz.txt', 'foo/bar/baz.txt') unless localfs.exists? 'foo/bar/baz.txt'
    </code></pre>            
    
    You can do:<pre><code>fs = Swineherd::Filesystem
    fs.cp('hdfs://foo/bar/baz.txt','foo/bar/baz.txt') unless fs.exists?('foo/bar/baz.txt')
    </code></pre>        

    Note: A path without a scheme is treated as a path on the local filesystem, or use the explicit file:// scheme for clarity.  The following are equivalent:

    <pre><code>fs.exists?('foo/bar/baz.txt')
    fs.exists?('file://foo/bar/baz.txt')
    </code></pre>

h3. Workflow

A workflow is built using rake @task@ objects that doing nothing more than run scripts. A workflow

* can be described with a directed dependency graph
* has an @id@ which is used to run its tasks idempotently. At the moment it is the responsibility of the running process (or human being) to choose a suitable id.
* manages intermediate outputs by using the @next_output@ and @latest_output@ methods. See the examples dir for usage.
* A workflow has a working directory in which all intermediate outputs go
** These are named according to the rake task that created them

h4. Config

 * In order to use the @S3Filesystem@, Swineherd requires AWS S3 access credentials.

 * In @~/swineherd.yaml@ or @/etc/swineherd.yaml@:

 <pre><code>aws:
   access_key: my_access_key
   secret_key: my_secret_key
 </code></pre>

 * Or just pass them in when creating the instance: 

 <pre><code>S3 = Swineherd::S3FileSystem.new(:access_key => "my_access_key",:secret_key => "my_secret_key")</code></pre>
