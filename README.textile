h1. SwineHerd

h3. Script

Given the Pig script @foo.pig.erb@:
<pre><code>fips = LOAD '<%= in_path %>' AS (fips_id:int,state_name:chararray);
DUMP fips;
</code></pre>

In the Ruby interpreter (irb):
<pre><code>require 'swineherd'
script = Swineherd::Script.new('foo.pig.erb',:in_path => 'fips_to_state.tsv')
script.run(:map_tasks => 20,:run_mode => 'local') #uses the PigRunner based upon the filename extension
</code></pre>

Hadoop and Pig Jobconf settings can be specified at runtime or at a system/user wide level:

@/etc/swineherd.yaml@
<pre><code>:map_tasks: 10</code></pre>
@~/swineherd.yaml@
<pre><code>:map_tasks:15</code></pre>

The above @script.run@ will interpolate the script variables,pass Hadoop and Pig settings through @PIG_OPTS@ and run the following script:

@/tmp/1325544799-4248-foo.pig@
<pre><code>fips = LOAD 'fips_to_state.tsv' AS (fips_id:int,state_name:chararray);
DUMP fips;
</code></pre>

With this command line:

<pre><code>ENV['PIG_OPTS'] = '-Dmapred.map.tasks=20'
/usr/local/share/pig/bin/pig -x local /tmp/1325544799-4248-foo.pig
</code></pre>

The same script can be run using the swineherd executable:

<pre><code>./swineherd --map_tasks=20 --run_mode=local --binding.in_path=fips_to_state.tsv foo.pig.erb</code></pre>

Script variables are specified using @--binding@


h4. Workflow:

A workflow is built using rake @task@ objects that doing nothing more than run scripts. A workflow

* can be described with a directed dependency graph
* has an @id@ which is used to run its tasks idempotently. At the moment it is the responsibility of the running process (or human being) to choose a suitable id.
* manages intermediate outputs by using the @next_output@ and @latest_output@ methods. See the examples dir for usage.
* A workflow has a working directory in which all intermediate outputs go
** These are named according to the rake task that created them

h4. FileSystem

    * @file@ - Local file system. Only thoroughly tested on Ubuntu Linux.
    * @hdfs@ - Hadoop distributed file system. Uses the Apache Hadoop 0.20 API. Requires Jruby.
    * @s3@   - Amazon Simple Storage System (s3).
    * @ftp@

    All filesystem abstractions implement the following core functions, many taken from the UNIX filesystem:

    * @mv@
    * @cp@
    * @cp_r@
    * @rm@
    * @rm_r@
    * @open@
    * @exists?@
    * @directory?@
    * @ls@
    * @ls_r@
    * @mkdir_p@

    Additionally, the S3 and HDFS abstractions implement functions for moving files to and from the local filesystem:

    * @copy_to_local@   (@get@)
    * @copy_from_local@ (@put@)

    
    Note: Since S3 is just a key-value store, it is difficult to preserve the notion of a directory.
    Therefore the @mkdir_p@ function has no purpose, as there cannot be empty directories. @mkdir_p@ currently only ensures that the bucket exists.
    This implies that the @directory?@ test only succeeds if the directory is non-empty, which clashes with the notion on the UNIX filesystem.

    
    The @Swineherd::Filesystem@ module can be used to @cp@ files between all abstractions, using schemed filepaths (hdfs://,s3://,file://).

    
    For example, instead of doing the following:<pre><code>hdfs = Swineherd::HdfsFilesystem.new
    localfs = Swineherd::LocalFileSystem.new
    hdfs.copy_to_local('foo/bar/baz.txt', 'foo/bar/baz.txt') unless localfs.exists? 'foo/bar/baz.txt'
    </code></pre>            

    
    You can do:<pre><code>fs = Swineherd::Filesystem
    fs.cp('hdfs://foo/bar/baz.txt','foo/bar/baz.txt') unless fs.exists?('foo/bar/baz.txt')
    </code></pre>        


    Note: A path without a scheme is treated as a path on the local filesystem, or use the explicit file:// scheme for clarity.  The following are equivalent:

    <pre><code>fs.exists?('foo/bar/baz.txt')
    fs.exists?('file://foo/bar/baz.txt')
    </code></pre>    


Using the filesystem:

Paths should be absolute.

<pre><code>
# get a new instance of local filesystem and write to it
localfs = FileSystem.get(:file)
localfs.open("mylocalfile", 'w') do |f|
  f.write("Writing a string to a local file")
end

# get a new instance of hadoop filesystem and write to it
hadoopfs = FileSystem.get(:hdfs)
hadoopfs.open("myhadoopfile", 'w') do |f|
  f.write("Writing a string to an hdfs file")
end

# get a new instance of s3 filesystem and write to it
access_key_id     = '1234abcd'
secret_access_key = 'foobar1234'
s3fs = FileSystem.get(:s3, accees_key_id, secret_access_key)
s3fs.mkpath 'mys3bucket' # bucket must exist
s3fs.open("mys3bucket/mys3file", 'w') do |f|
  f.write("Writing a string to an s3 file")
end
</code></pre>

h3. TODO

* next task in a workflow should NOT run if the previous step failed
** this is made difficult by the fact that, sometimes?, when a pig script fails it still returns a 0 exit status
** same for wukong scripts
* add a @job@ object that implements a @not_if@ function. this way a @workflow@ will be constructed of @job@ objects
** a @job@ will do nothing more than execute the ruby code in it's (run?) block, unless @not_if@ is true
** this way we can put @script@ objects inside a @job@ and only run under certain conditions that the user specifies when
   they create the @job@
* implement ftp filesystem interfaces
